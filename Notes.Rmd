---
title: "Review Notes"
author: "Alan T. Arnholt"
date: "1/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Z & t 

$$Z = \frac{stat  - \mu_{stat}}{\sigma_{stat}}$$ 

$$t = \frac{stat  - \mu_{stat}}{\hat{\sigma}_{stat}}$$ 

## Consider the distribution of $\bar{X}$

$$\bar{X} \sim \text{Something}\left(\mu_{\bar{x}} = \mu, \sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}\right)$$

### Is 30 a magic number?

Read [http://www.seio.es/BBEIO/BEIOVol34Num3/files/assets/basic-html/page-13.html](http://www.seio.es/BBEIO/BEIOVol34Num3/files/assets/basic-html/page-13.html)

### How to find $z$ and $t$ values

Notation: $z_{\alpha}$, the $\alpha$ is the area to the left of the $z$ value.

Examples:

$z_{0.05}$ = `r qnorm(0.05, 0, 1)` found using the command `qnorm(0.05, 0, 1)` or just `qnorm(0.05)`.

$z_{0.90}$ = `r qnorm(0.90)` found with `qnorm(0.90)`.

Finding the area to the right of a value is found using `p_distName`.  For example, to find $P(Z \leq 1.2816) =$ which is `r pnorm(1.2816)` use `pnorm(1.2816)`. 

$P(t_{12} \leq 2.0)=$ which is `r pt(2.0, 12)` use `pt(2.0, 12)`.

$t_{0.975, 12} = `r qt(.975, 12)`$ use `qt(0.975, 12)`.

## Confidence interval for the Mean ($\mu$)

Derive in class...

$$P\left(t_{\alpha/2, n - 1} \leq \frac{\bar{x} - \mu}{\sigma/\sqrt{n}} \leq t_{1 - \alpha/2, n - 1}\right) = 1- \alpha$$

$$CI_{1 - \alpha}(\mu)= \left[\bar{x} - t_{1 - \alpha/2, n - 1}\cdot s/\sqrt{n}, \bar{x} + t_{1 - \alpha/2, n - 1}\cdot s/\sqrt{n}\right]$$

* What are the assumptions to use said interval?
 - Independence assumption
 - Nearly Normal condition
 
## Example

A medical researcher measured the temperature of a sample of 52 randomly selected adults.  The results are in the data frame `temp`.

```{r}
temp <- read.csv("../NT.csv")
head(temp)
# The long way
temps <- temp$Temp
(n <- sum(!is.na(temps)))
(ybar <- mean(temps))
(t_crit <- qt(.975, 52-1))
(S <- sd(temps))
(CI <- ybar + c(-1, 1)*t_crit*S/sqrt(n))
# Short cut 95% CI
t.test(temps, conf.level = 0.95)$conf
```

* How do we interpret the confidence interval?

We are $95%$ confident in the method we used to construct the confidence interval...That is, if we were to take numerous samples (say 10,000), and construct confidence intervals using the above formula, we would expect $95%$ of said confidence intervals to contain the true parameter.

This often gets abbreviated to something along the lines....We are 95% confident that the true parameter falls in the computed interval.  Note that the confidence is not in the interval but in the method used to construct said interval!

* What do we do if the assumptions are not satisfied for using the CI? $\rightarrow$ Bootstrap!

Bootstrap---take repeated samples of the same size as the original sample with replacement.

```{r}
B <- 10000
bsm <- numeric(B)
for(i in 1:B){
  bss <- sample(temps, size = 52, replace = TRUE)
  bsm[i] <- mean(bss)
}
hist(bsm)
# Bootstrap Percentile 95% CI
(CI_bsm <- quantile(bsm, probs = c(0.025, 0.975)))
```

## Hypothesis Testing

1. Write the Null and Alternate hypothesis
$H_0: \mu = 98.6$ versus $H_A: \mu < 98.6$

2. Specify the distribution of the statistic and test statistic.
$\bar{x}\sim N\left(\mu = 98.6, \frac{\sigma}{\sqrt{52}}\right)$,
$t_{51} = \frac{\bar{x} - 98.6}{s/\sqrt{52}}$

3. Compute the standardized test statistic:

$t_{standardized} = \frac{98.28462 - 98.6}{0.6823/\sqrt{52}} = `r t.test(temps, mu = 98.6)$stat`$.

4. Compute the p-value

$P(t_{51} \leq -3.3329) = `r pt(-3.3329, 51)`$.

```{r}
t.test(temps, mu = 98.6, alternative = "less")
```
5. English Conclusion---always uses the alternative hypothesis---either find evidence to suggest $H_A$ or insufficient evidence to suggest $H_A$.

In this case, there is sufficient evidence to suggest the true average temperature for adults is less than 98.6 degrees Fahrenheit.

____________

## Bootstrap test of one parameter

Find the sample mean, $\bar{x}$, from your data.  Add $(\mu_0 - \bar{x})$ in your sample to shift the center of this sample to $\mu_0$. Bootstrap this shifted sample B times.  Apply a statistic to each bootstrapped sample.  Use the bootstrap sampling distribution to compute your p-value.

```{r}
(xbar <- mean(temps))
(delta <- 98.6 - xbar)
shifted_temps <- temps + delta
mean(shifted_temps)
B <- 10000
bsm <- numeric(B)
for(i in 1:B){
  bss <- sample(shifted_temps, size = 52, replace = TRUE)
  bsm[i] <- mean(bss)
}
hist(bsm)
pvalue <- mean(bsm <= xbar)
pvalue
```

